{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBM409 Assignment 2 - Detection of Fake News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question1:\n",
    "Assume that you have a data consisting of $ x_1 $ , $ x_2 $ ,..., $ x_m $  where each $ x_i $ represents\n",
    "a single real value, which means you have m instances in data and each instance\n",
    "has an single real-valued attribute. Assume also that the given data has random\n",
    "uniform distribution between $ −w $ and $ w $. You are expected to find the maximum\n",
    "likelihood estimate of w with respect to the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ * $ Specify a likelihood function $F(w)$  <br />  <br />\n",
    "$F(w) = \\prod_{i=1}^{m}P_w(x_i) $\n",
    "\n",
    "It is uniformly distributed\n",
    "\n",
    "$P_w(x_i) =  \\Bigg\\{ \\begin{array}{c}\n",
    "\\frac{1}{2w},\\quad \\quad if x\\in[-w,w], \\\\\n",
    "0,\\quad \\text{otherwise.} \\\\\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ * $ Specify the maximum likelihood estimate for w. Consider your answer based on the likelihood function you state.\n",
    "\n",
    "$ F(w) = \\prod_{i=1}^{m}P_w(x_i) = \\prod_{i=1}^{m}\\frac{1}{2w} = \\frac{1}{2w^{m}} = 2w^{-m} $\n",
    "\n",
    "We carry out ln function both side\n",
    "\n",
    "$InF(w) = -mIn2w$\n",
    "\n",
    "$\\frac{d \\mathbf{InF(w)}}{d \\mathbf{w}} = \\frac{-m}{2w} $\n",
    "\n",
    "The result is less than zero when w is bigger than zero\n",
    "\n",
    "$F(w)$ is a decreasing function in bounds and otherwise it will be zero. This gives,\n",
    "\n",
    "$ w = max(| min_{1≤i≤m} x_i|, | max_{1≤i≤m} x_i|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ * $ Assume that this time you are given a labelled data $(x_i, y_i)$, where $y_i$ is 1 or 0. Remember that a generative classifier will try to model $P(y)$ and $P(x|y)$. Define an example dataset the generative classifier utilizing the model you defined above for each $P(x|y)$ could not perform well on.\n",
    "\n",
    "Lets say there are x positive values in range $ [0, N] $ and x negative values in range $ [-N, 0] $. The Maximum Likelihood Estimate will give the value M for w. So we will classify them based on previous classes. The issue is the bias of the classifier. it is not symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Remember that a discriminative classifier will try to model P(y|x). State that whether you can classify the labelled data given in previously part using such a discriminative classifier or not. If your answer is answer is yes, then please also show that what your suggested classifier looks like.\n",
    "\n",
    "Yes. Threshold value can be selected. The value of x can be classified according to whether < or > case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "Fill the blanks with T (True) or F (False) for the statements given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ * $ Maximum likelihood estimation provides not only point estimation, but a distribution information of the parameters estimated. (F)\n",
    "\n",
    "$ * $ Maximum likelihood and Bayesian approachs for parameter estimation perform well with low-dimensional dataset with many training examples while their performance is bad on high-dimensional dataset with few training examples. (T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "Consider that you are given the dataset in the table above consisting of boolean variables x, y and z and a single boolean output variable C. Suppose that the Naive Bayes classifier is going to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ * $ Specify the value of $P(C = 1|x = 1, y = 1, z = 0)$. Show your solution step by step.\n",
    "\n",
    "$ P(C = 1|x = 1, y = 1, z = 0) = \\frac{P(C = 1,x = 1, y = 1, z = 0)} {P(x = 1, y = 1, z = 0)} = \\frac{P(C=1) * P(x = 1|C = 1) * P(y = 1|C = 1) * P(z = 0|C = 1)}{P(x = 1 , y = 1 , z = 0 , C = 1) + P(x = 1 , y = 1 , z = 0 , C = 0)} = \\frac{1}{4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ * $ Specify the value of $P(C = 0|x = 1, y = 1)$. Show your solution step by step. \n",
    "\n",
    "$P(C = 0|x = 1, y = 1) = \\frac {P(C = 0 , x = 1 , y = 1)}{P(x = 1 , y = 1)} = \\frac {P(C= 0) * P(x = 1|C = 0) * P(y = 1|C = 0)}{P(x = 1 , y = 1 , C = 0) + P(x = 1 , y = 1 , C = 1)} = \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that the Joint Naive Bayes classifier is used for the options below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ * $ Specify the value of $P(C = 1|x = 1, y = 1, z = 0)$. Show your solution step by step.\n",
    "\n",
    "$P(C = 1|x = 1, y = 1, z = 0) = \\frac{num(C = 1, x = 1, y = 1, z = 0)}{num(x = 1, y = 1, z = 0)}$\n",
    "\n",
    "$num(C = 1, x = 1, y = 1, z = 0) = 0 $\n",
    "\n",
    "$P(C = 1|x = 1, y = 1, z = 0) = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ * $ Specify the value of $ P(C = 0|x = 1, y = 1)$. Show your solution step by step.\n",
    "\n",
    "$ P(C = 0|x = 1, y = 1) = \\frac{num(C = 0, x = 1, y = 1)}{num(x = 1, y = 1)}$\n",
    "\n",
    "$ P(C = 0|x = 1, y = 1) = \\frac{1}{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "Assume that you have three variables, which are A, B and C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ * $ Suppose that you have the following informations $P(C|A) = 0.7$ and $P(C|B) = 0.4$. State that whether you can compute $P(C|A, B)$ with the informations given previously or not. Besides show your solution if you can and explain the reason if you can not. \n",
    "\n",
    "$P(C|A, B) = \\frac {P(C,A,B)}{P(A,B)} = \\frac {P(C)*P(A|C)*P(B|C)}{P(A,B)}$\n",
    "\n",
    "$P(A|C) = \\frac {P(C|A)*P(A)}{P(C)}$\n",
    "\n",
    "$P(B|C) = \\frac {P(C|B)*P(B)}{P(C)}$\n",
    "\n",
    "We need the information of $P(A),P(B),P(C)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ * $ Suppose that besides two informations above, $P(A) = 0.3$ and $P(B) = 0.5$ informations are given. State that whether you can compute $P(C|A, B)$ with the informations given previously or not. Besides show your solution if you can and explain the reason if you can not.\n",
    "\n",
    "$P(C|A, B) = \\frac {P(C,A,B)}{P(A,B)} = \\frac {P(C)*P(A|C)*P(B|C)}{P(A,B)}$\n",
    "\n",
    "$P(A|C) = \\frac {P(C|A)*P(A)}{P(C)} \\Longrightarrow \\frac {0.21}{P(C)}$\n",
    "\n",
    "$P(B|C) = \\frac {P(C|B)*P(B)}{P(C)}  \\Longrightarrow \\frac {0.20}{P(C)}$\n",
    "\n",
    "$P(A,B)= P(A|B)*P(B) = P(B|A)*P(A)$\n",
    "\n",
    "We need the information of $P(A|B), P(B|A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ * $ Finally assume that you have only informations, which are $P(C, A) = 0.2$, $P(A) = 0.3$ and $P(B) = 1$. State that whether you can compute $P(C|A, B)$ with the informations given previously or not. Besides show your solution if you can and explain the reason if you can not.\n",
    "\n",
    "$P(C|A, B) = \\frac {P(C,A,B)}{P(A,B)} = \\frac {P(C)*P(A|C)*P(B|C)}{P(A,B)}$\n",
    "\n",
    "$P(A,B) = P(A) $ in case of $P(B)=1$\n",
    "\n",
    "$P(B|C) = \\frac {P(C|B)*P(B)}{P(C)} = \\frac {P(C) * 1  }{P(C)} = P(B|C) = 1 $\n",
    "\n",
    "$P(C|A, B) = \\frac {P(C)*P(A|C)}{P(A)}) = \\frac {P(A,C)}{P(A)} = \\frac{0.2} {0.3} = \\frac {2}{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the likelihood probability using laplace smoothing method to handle the situation of the word never presence in text\n",
    "def calc_cond_prob(numOfOccurences, numOfWords, uniqueWords):\n",
    "    return float(numOfOccurences + 1) / (numOfWords + uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fuction calculates naive bayes probabilities of test news and returns a list of predictions\n",
    "# It also implement log probability method to prevent numerical underflow\n",
    "def naive_bayes(test_news_vectorizer, countWordInTestNews, cond_probs_fake, cond_probs_real, fakeWords,\n",
    "                realWords, length_fake, length_real, uniqueWords, totalFakeProb, totalRealProb):\n",
    "    labels_of_test = list()\n",
    "    content_of_test_element = test_news_vectorizer.inverse_transform(countWordInTestNews)\n",
    "    \n",
    "    # for fake news probability\n",
    "    listOfFakeProbs = list()\n",
    "    for i in range(len(content_of_test_element)):   # for each line\n",
    "        fakeProb = 1.0\n",
    "        for j in content_of_test_element[i]:   # for each word in line\n",
    "            if j in fakeWords:                 # if it is a learned word\n",
    "                fakeProb *= cond_probs_fake[j]\n",
    "            else:\n",
    "                fakeProb *= calc_cond_prob(0, length_fake, uniqueWords)\n",
    "        fakeProb *= totalFakeProb\n",
    "        listOfFakeProbs.append(np.log(fakeProb))\n",
    "\n",
    "    # for real news probability\n",
    "    listOfRealProbs = list()\n",
    "    for i in range(len(content_of_test_element)):   # for each line\n",
    "        realProb = 1.0\n",
    "        for j in content_of_test_element[i]:     # for each word in line\n",
    "            if j in realWords:                   # if it is a learned word\n",
    "                realProb *= cond_probs_real[j]\n",
    "            else:\n",
    "                realProb *= calc_cond_prob(0, length_real, uniqueWords)\n",
    "                realProb *= totalRealProb\n",
    "        listOfRealProbs.append(np.log(realProb))\n",
    "\n",
    "    for i in range(len(listOfFakeProbs)):\n",
    "        labels_of_test.append(1 if listOfFakeProbs[i] > listOfRealProbs[i] else 0)\n",
    "\n",
    "    return labels_of_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the conditional probability, it returns a dictionary whose keys are words and values are probability\n",
    "def cond_prob(words, totalCountsOfWords, length, uniqueWordsNumber):\n",
    "    cond_probs = dict()\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        cond_probs[word] = calc_cond_prob(totalCountsOfWords[i], length, uniqueWordsNumber)\n",
    "    return cond_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this code part, dataframe is read from file and splitted to train and test data by ratio of 70%\n",
    "data = pd.read_csv(\"fake_news_train.csv\")\n",
    "train_news = data[:round(len(data)*0.7)]\n",
    "test_news = data[round(len(data)*0.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I handled the NaN value by changing the absent parts with relevant message\n",
    "train_news.iloc[:,1].fillna(\"no_title\", inplace=True)\n",
    "train_news.iloc[:,2].fillna(\"no_author\", inplace=True)\n",
    "train_news.iloc[:,3].fillna(\"no_text\", inplace=True)\n",
    "\n",
    "test_news.iloc[:,1].fillna(\"no_title\", inplace=True)\n",
    "test_news.iloc[:,2].fillna(\"no_author\", inplace=True)\n",
    "test_news.iloc[:,3].fillna(\"no_text\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part splits the data to fake and real news\n",
    "fake_news = train_news[train_news['label'] == 1]\n",
    "real_news = train_news[train_news['label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part creates a matrix whose rows hold the news and columns hold the number of occurences using CountVectorizer function\n",
    "# It stores the information only from title column\n",
    "train_news_vectorizer = CountVectorizer()\n",
    "newscv = train_news_vectorizer.fit_transform(train_news['title'])\n",
    "countWordInTrainNews = newscv.toarray()\n",
    "\n",
    "test_news_vectorizer = CountVectorizer()\n",
    "test_newscv = test_news_vectorizer.fit_transform(test_news['title'])\n",
    "countWordInTestNews = test_newscv.toarray()\n",
    "\n",
    "fake_vectorizer = CountVectorizer()\n",
    "fakeNewsCv = fake_vectorizer.fit_transform(fake_news['title'])\n",
    "countWordInFake = fakeNewsCv.toarray()\n",
    "\n",
    "real_vectorizer = CountVectorizer()\n",
    "realNewscv = real_vectorizer.fit_transform(real_news['title'])\n",
    "countWordInReal = realNewscv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables holds the word data in the order of counts of names\n",
    "train_newsWords = train_news_vectorizer.get_feature_names()\n",
    "test_newsWords = test_news_vectorizer.get_feature_names()\n",
    "fakeWords = fake_vectorizer.get_feature_names()\n",
    "realWords = real_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part creates a structure that holds the number of occurences each word in entire data \n",
    "totalCountsOfWordsInTrainNews = np.sum(countWordInTrainNews, axis=0)\n",
    "totalCountsOfWordsInTestNews = np.sum(countWordInTestNews, axis=0)\n",
    "totalCountsOfWordsInFake = np.sum(countWordInFake, axis=0)\n",
    "totalCountsOfWordsInReal = np.sum(countWordInReal, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1\n",
    "Next line part analyze the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word - expands - is 5 times in real news. Also it is never in fake news\n",
      "The word - cotton - is 6 times in real news. Also it is never in fake news\n",
      "The word - criticizes - is 5 times in real news. Also it is never in fake news\n",
      "The word - incredible - is 6 times in fake news. Also it is never in real news\n",
      "The word - totally - is 10 times in fake news. Also it is never in real news\n",
      "The word - raqqa - is 13 times in fake news. Also it is never in real news\n"
     ]
    }
   ],
   "source": [
    "# Reorganizing the data and creates the list of words whose presence are only in real or fake words\n",
    "fakeWordSet = set(fakeWords)\n",
    "realWordSet = set(realWords)\n",
    "wordsOnlyReal = realWordSet - fakeWordSet\n",
    "wordsOnlyFake = fakeWordSet - realWordSet\n",
    "\n",
    "# Analyzing words and finding the words that is pure to a label, also calculating how many times it occurs\n",
    "i = 0\n",
    "while i < 3:\n",
    "    word = choice(list(wordsOnlyReal))\n",
    "    indexNum = realWords.index(word)\n",
    "    if totalCountsOfWordsInReal[indexNum] >= 5:\n",
    "        print(\"The word -\", word, \"- is\", totalCountsOfWordsInReal[indexNum],\n",
    "              \"times in real news. Also it is never in fake news\")\n",
    "        i += 1\n",
    "i = 0\n",
    "while i < 3:\n",
    "    word = choice(list(wordsOnlyFake))\n",
    "    indexNum = fakeWords.index(word)\n",
    "    if totalCountsOfWordsInFake[indexNum] >= 5:\n",
    "        print(\"The word -\", word, \"- is\", totalCountsOfWordsInFake[indexNum],\n",
    "              \"times in fake news. Also it is never in real news\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part creates a matrix whose rows hold the news and columns hold the number of occurences using CountVectorizer function\n",
    "# It takes data from text part and uses min_df and max_df parameter to crop some of data to be able to make program efficient in running time.\n",
    "unigram_train_news_vectorizer = CountVectorizer(min_df=0.025, max_df=0.975)\n",
    "unigram_newscv = unigram_train_news_vectorizer.fit_transform(train_news['text'])\n",
    "unigram_countWordInTrainNews = unigram_newscv.toarray()\n",
    "\n",
    "unigram_test_news_vectorizer = CountVectorizer(min_df=0.025, max_df=0.975)\n",
    "unigram_test_newscv = unigram_test_news_vectorizer.fit_transform(test_news['text'])\n",
    "unigram_countWordInTestNews = unigram_test_newscv.toarray()\n",
    "\n",
    "unigram_fake_vectorizer = CountVectorizer(min_df=0.025, max_df=0.975)\n",
    "unigram_fakeNewsCv = unigram_fake_vectorizer.fit_transform(fake_news['text'])\n",
    "unigram_countWordInFake = unigram_fakeNewsCv.toarray()\n",
    "\n",
    "unigram_real_vectorizer = CountVectorizer(min_df=0.025, max_df=0.975)\n",
    "unigram_realNewscv = unigram_real_vectorizer.fit_transform(real_news['text'])\n",
    "unigram_countWordInReal = unigram_realNewscv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables holds the word data in the order of counts of names\n",
    "unigram_train_newsWords = unigram_train_news_vectorizer.get_feature_names()\n",
    "unigram_test_newsWords = unigram_test_news_vectorizer.get_feature_names()\n",
    "unigram_fakeWords = unigram_fake_vectorizer.get_feature_names()\n",
    "unigram_realWords = unigram_real_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part creates a structure that holds the number of occurences each word in entire data \n",
    "unigram_totalCountsOfWordsInTrainNews = np.sum(unigram_countWordInTrainNews, axis=0)\n",
    "unigram_totalCountsOfWordsInTestNews = np.sum(unigram_countWordInTestNews, axis=0)\n",
    "unigram_totalCountsOfWordsInFake = np.sum(unigram_countWordInFake, axis=0)\n",
    "unigram_totalCountsOfWordsInReal = np.sum(unigram_countWordInReal, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This parts calculates P(Fake) and P(Real)\n",
    "totalFakeProb = fake_news.shape[0] / train_news.shape[0]\n",
    "totalRealProb = real_news.shape[0] / train_news.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in a label\n",
    "unigram_length_fake, unigram_length_real = 0, 0\n",
    "for i in range(len(unigram_fakeWords)):\n",
    "    unigram_length_fake += unigram_totalCountsOfWordsInFake[i]\n",
    "\n",
    "for i in range(len(unigram_realWords)):\n",
    "    unigram_length_real += unigram_totalCountsOfWordsInReal[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is feasible to predict whether a headline is real or fake news from words that\n",
    "appear in the headline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2\n",
    "Implementing Naive Bayes with unigram and bigram\n",
    "CountVectorizer is set in unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates conditional probabilities for fake and real news apartly\n",
    "unigram_cond_probs_fake = cond_prob(unigram_fakeWords, unigram_totalCountsOfWordsInFake, unigram_length_fake, len(unigram_train_newsWords))\n",
    "unigram_cond_probs_real = cond_prob(unigram_realWords, unigram_totalCountsOfWordsInReal, unigram_length_real, len(unigram_train_newsWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yusuf Emre\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Yusuf Emre\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "# Carrying out the Naive Bayes Formula\n",
    "unigram_labels_of_test = naive_bayes(unigram_test_news_vectorizer, unigram_countWordInTestNews, unigram_cond_probs_fake, unigram_cond_probs_real, unigram_fakeWords, unigram_realWords,\n",
    "            unigram_length_fake, unigram_length_real, len(unigram_train_newsWords), totalFakeProb, totalRealProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.379807692307686\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Accuracy of unigram part\n",
    "numOfTruePrediction = 0\n",
    "for i in range(len(unigram_labels_of_test)):\n",
    "    if test_news.iloc[i, 4] == unigram_labels_of_test[i]:\n",
    "        numOfTruePrediction += 1\n",
    "unigram_accuracy = 100*(numOfTruePrediction/float(len(unigram_labels_of_test)))\n",
    "print(unigram_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part creates a matrix whose rows hold the news and columns hold the number of occurences using CountVectorizer function in bigram form\n",
    "# It takes data from text part and uses min_df and max_df parameter to crop some of data to be able to make program efficient in running time.\n",
    "bigram_train_news_vectorizer = CountVectorizer(ngram_range=(2,2), min_df=0.025, max_df=0.975)\n",
    "bigram_newscv = bigram_train_news_vectorizer.fit_transform(train_news['text'])\n",
    "bigram_countWordInTrainNews = bigram_newscv.toarray()\n",
    "\n",
    "bigram_test_news_vectorizer = CountVectorizer(ngram_range=(2,2), min_df=0.025, max_df=0.975)\n",
    "bigram_test_newscv = bigram_test_news_vectorizer.fit_transform(test_news['text'])\n",
    "bigram_countWordInTestNews = bigram_test_newscv.toarray()\n",
    "\n",
    "bigram_fake_vectorizer = CountVectorizer(ngram_range=(2,2), min_df=0.025, max_df=0.975)\n",
    "bigram_fakeNewsCv = bigram_fake_vectorizer.fit_transform(fake_news['text'])\n",
    "bigram_countWordInFake = bigram_fakeNewsCv.toarray()\n",
    "\n",
    "bigram_real_vectorizer = CountVectorizer(ngram_range=(2,2), min_df=0.025, max_df=0.975)\n",
    "bigram_realNewscv = bigram_real_vectorizer.fit_transform(real_news['text'])\n",
    "bigram_countWordInReal = bigram_realNewscv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables holds the word data in the order of counts of names in bigram form\n",
    "bigram_train_newsWords = bigram_train_news_vectorizer.get_feature_names()\n",
    "bigram_test_newsWords = bigram_test_news_vectorizer.get_feature_names()\n",
    "bigram_fakeWords = bigram_fake_vectorizer.get_feature_names()\n",
    "bigram_realWords = bigram_real_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part creates a structure that holds the number of occurences each word in entire data in bigram form\n",
    "bigram_totalCountsOfWordsInTrainNews = np.sum(bigram_countWordInTrainNews, axis=0)\n",
    "bigram_totalCountsOfWordsInTestNews = np.sum(bigram_countWordInTestNews, axis=0)\n",
    "bigram_totalCountsOfWordsInFake = np.sum(bigram_countWordInFake, axis=0)\n",
    "bigram_totalCountsOfWordsInReal = np.sum(bigram_countWordInReal, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in a label in bigram form\n",
    "bigram_length_fake, bigram_length_real = 0, 0\n",
    "for i in range(len(bigram_fakeWords)):\n",
    "    bigram_length_fake += bigram_totalCountsOfWordsInFake[i]\n",
    "\n",
    "for i in range(len(bigram_realWords)):\n",
    "    bigram_length_real += bigram_totalCountsOfWordsInReal[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates conditional probabilities for fake and real news apartly\n",
    "bigram_cond_probs_fake = cond_prob(bigram_fakeWords, bigram_totalCountsOfWordsInFake, bigram_length_fake, len(bigram_train_newsWords))\n",
    "bigram_cond_probs_real = cond_prob(bigram_realWords, bigram_totalCountsOfWordsInReal, bigram_length_real, len(bigram_train_newsWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yusuf Emre\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Yusuf Emre\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "# Carrying out the Naive Bayes Formula\n",
    "labels_of_test = naive_bayes(bigram_test_news_vectorizer, bigram_countWordInTestNews, bigram_cond_probs_fake, bigram_cond_probs_real, bigram_fakeWords, bigram_realWords,\n",
    "            bigram_length_fake, bigram_length_real, len(bigram_train_newsWords), totalFakeProb, totalRealProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.08573717948718\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Accuracy of bigram part\n",
    "numOfTruePrediction = 0\n",
    "for i in range(len(labels_of_test)):\n",
    "    if test_news.iloc[i, 4] == labels_of_test[i]:\n",
    "        numOfTruePrediction += 1\n",
    "bigram_accuracy = 100*(numOfTruePrediction/float(len(labels_of_test)))\n",
    "print(bigram_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3\n",
    "Analyzing effect of the words on prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In  this part, I used TFIDF to take the the weights of the words. In other words, it returns the importance values of the words.\n",
    "tfidf_train_news_vectorizer = TfidfTransformer()\n",
    "tfidf_newscv = tfidf_train_news_vectorizer.fit_transform(unigram_newscv)\n",
    "tfidf_countWordInTrainNews = tfidf_newscv.toarray()\n",
    "\n",
    "tfidf_test_news_vectorizer = TfidfTransformer()\n",
    "tfidf_test_newscv = tfidf_test_news_vectorizer.fit_transform(unigram_test_newscv)\n",
    "tfidf_countWordInTestNews = tfidf_test_newscv.toarray()\n",
    "\n",
    "tfidf_fake_vectorizer = TfidfTransformer()\n",
    "tfidf_fakeNewsCv = tfidf_fake_vectorizer.fit_transform(unigram_fakeNewsCv)\n",
    "tfidf_countWordInFake = tfidf_fakeNewsCv.toarray()\n",
    "\n",
    "tfidf_real_vectorizer = TfidfTransformer()\n",
    "tfidf_realNewscv = tfidf_real_vectorizer.fit_transform(unigram_realNewscv)\n",
    "tfidf_countWordInReal = tfidf_realNewscv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_totalCountsOfWordsInTrainNews = np.sum(tfidf_countWordInTrainNews, axis=0)\n",
    "tfidf_totalCountsOfWordsInTestNews = np.sum(tfidf_countWordInTestNews, axis=0)\n",
    "tfidf_totalCountsOfWordsInFake = np.sum(tfidf_countWordInFake, axis=0)\n",
    "tfidf_totalCountsOfWordsInReal = np.sum(tfidf_countWordInReal, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 10 words whose presence most strongly predicts that the news is real.\n",
      "defecting\n",
      "curtailed\n",
      "deaths\n",
      "booming\n",
      "commish\n",
      "booed\n",
      "bias\n",
      "battered\n",
      "cajole\n",
      "crist\n",
      "List of 10 words whose absence most strongly predicts that the news is real.\n",
      "cheers\n",
      "aussie\n",
      "ancient\n",
      "classroom\n",
      "astronauts\n",
      "cotton\n",
      "contributed\n",
      "answers\n",
      "costing\n",
      "backfire\n",
      "List of 10 words whose presence most strongly predicts that the news is fake.\n",
      "bombing\n",
      "bannock\n",
      "ascension\n",
      "banker\n",
      "civilizations\n",
      "backup\n",
      "3d\n",
      "bodyguard\n",
      "clever\n",
      "ck\n",
      "List of 10 words whose absence most strongly predicts that the news is fake.\n",
      "borders\n",
      "1975\n",
      "bastion\n",
      "arcturian\n",
      "cage\n",
      "boris\n",
      "aids\n",
      "balistique\n",
      "arabie\n",
      "antarctic\n"
     ]
    }
   ],
   "source": [
    "# The words whose presence has more weight to predict correctly in real news help us classifying.\n",
    "# Likewise, the words whose presence has less weight to predict correctly in fake news help us avoiding misclassification.\n",
    "\n",
    "indices_sorted_real_news = np.argsort(tfidf_totalCountsOfWordsInReal)\n",
    "indices_sorted_fake_news = np.argsort(tfidf_totalCountsOfWordsInFake)\n",
    "\n",
    "print(\"List of 10 words whose presence most strongly predicts that the news is real.\")\n",
    "for i in indices_sorted_real_news[-10:]:\n",
    "    if realWords[i] == 'no_title':\n",
    "        print(realWords[indices_sorted_real_news[-11]])\n",
    "    print(realWords[i])\n",
    "    \n",
    "print(\"List of 10 words whose absence most strongly predicts that the news is real.\")\n",
    "for i in indices_sorted_real_news[:10]:\n",
    "    if realWords[i] == 'no_title':\n",
    "        print(realWords[indices_sorted_real_news[11]])\n",
    "    print(realWords[i])\n",
    "    \n",
    "print(\"List of 10 words whose presence most strongly predicts that the news is fake.\")\n",
    "for i in indices_sorted_fake_news[-10:]:\n",
    "    if fakeWords[i] == 'no_title':\n",
    "        print(fakeWords[indices_sorted_fake_news[-11]])\n",
    "    print(fakeWords[i])\n",
    "    \n",
    "print(\"List of 10 words whose absence most strongly predicts that the news is fake.\")\n",
    "for i in indices_sorted_fake_news[:10]:\n",
    "    if fakeWords[i] == 'no_title':\n",
    "        print(fakeWords[indices_sorted_fake_news[11]])\n",
    "    print(fakeWords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_length_fake, tfidf_length_real = 0, 0\n",
    "for i in range(tfidf_countWordInFake.shape[1]):\n",
    "    tfidf_length_fake += tfidf_totalCountsOfWordsInFake[i]\n",
    "\n",
    "for i in range(tfidf_countWordInReal.shape[1]):\n",
    "    tfidf_length_real += tfidf_totalCountsOfWordsInReal[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_cond_probs_fake = cond_prob(unigram_fakeWords, tfidf_totalCountsOfWordsInFake, tfidf_length_fake, tfidf_countWordInTrainNews.shape[1])\n",
    "tfidf_cond_probs_real = cond_prob(unigram_realWords, tfidf_totalCountsOfWordsInReal, tfidf_length_real, tfidf_countWordInTrainNews.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yusuf Emre\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Yusuf Emre\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "labels_of_test = naive_bayes(test_news_vectorizer, tfidf_countWordInTestNews, tfidf_cond_probs_fake, tfidf_cond_probs_real, unigram_fakeWords, unigram_realWords,\n",
    "            tfidf_length_fake, tfidf_length_real, tfidf_countWordInTrainNews.shape[1], totalFakeProb, totalRealProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.71554487179487\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Accuracy of tfidf part\n",
    "numOfTruePrediction = 0\n",
    "for i in range(len(labels_of_test)):\n",
    "    if test_news.iloc[i, 4] == labels_of_test[i]:\n",
    "        numOfTruePrediction += 1\n",
    "tfidf_accuracy = 100*(numOfTruePrediction/float(len(labels_of_test)))\n",
    "print(tfidf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation part 2 with only title data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part creates a matrix whose rows hold the news and columns hold the number of occurences using CountVectorizer function\n",
    "train_news_vectorizer = CountVectorizer()\n",
    "newscv = train_news_vectorizer.fit_transform(train_news['title'])\n",
    "countWordInTrainNews = newscv.toarray()\n",
    "\n",
    "test_news_vectorizer = CountVectorizer()\n",
    "test_newscv = test_news_vectorizer.fit_transform(test_news['title'])\n",
    "countWordInTestNews = test_newscv.toarray()\n",
    "\n",
    "fake_vectorizer = CountVectorizer()\n",
    "fakeNewsCv = fake_vectorizer.fit_transform(fake_news['title'])\n",
    "countWordInFake = fakeNewsCv.toarray()\n",
    "\n",
    "real_vectorizer = CountVectorizer()\n",
    "realNewscv = real_vectorizer.fit_transform(real_news['title'])\n",
    "countWordInReal = realNewscv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables holds the word data in the order of counts of names\n",
    "train_newsWords = train_news_vectorizer.get_feature_names()\n",
    "test_newsWords = test_news_vectorizer.get_feature_names()\n",
    "fakeWords = fake_vectorizer.get_feature_names()\n",
    "realWords = real_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part creates a structure that holds the number of occurences each word in entire data \n",
    "totalCountsOfWordsInTrainNews = np.sum(countWordInTrainNews, axis=0)\n",
    "totalCountsOfWordsInTestNews = np.sum(countWordInTestNews, axis=0)\n",
    "totalCountsOfWordsInFake = np.sum(countWordInFake, axis=0)\n",
    "totalCountsOfWordsInReal = np.sum(countWordInReal, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in a label\n",
    "length_fake, length_real = 0, 0\n",
    "for i in range(len(fakeWords)):\n",
    "    length_fake += totalCountsOfWordsInFake[i]\n",
    "\n",
    "for i in range(len(realWords)):\n",
    "    length_real += totalCountsOfWordsInReal[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_probs_fake = cond_prob(fakeWords, totalCountsOfWordsInFake, length_fake, len(train_newsWords))\n",
    "cond_probs_real = cond_prob(realWords, totalCountsOfWordsInReal, length_real, len(train_newsWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_of_test = naive_bayes(test_news_vectorizer, countWordInTestNews, cond_probs_fake, cond_probs_real, fakeWords, realWords,\n",
    "            length_fake, length_real, len(train_newsWords), totalFakeProb, totalRealProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.90384615384616\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Accuracy of unigram part\n",
    "numOfTruePrediction = 0\n",
    "for i in range(len(labels_of_test)):\n",
    "    if test_news.iloc[i, 4] == labels_of_test[i]:\n",
    "        numOfTruePrediction += 1\n",
    "unigram_accuracy = 100*(numOfTruePrediction/float(len(labels_of_test)))\n",
    "print(unigram_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part creates a matrix whose rows hold the news and columns hold the number of occurences using CountVectorizer function in bigram form\n",
    "bigram_train_news_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "bigram_newscv = bigram_train_news_vectorizer.fit_transform(train_news['title'])\n",
    "bigram_countWordInTrainNews = bigram_newscv.toarray()\n",
    "\n",
    "bigram_test_news_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "bigram_test_newscv = bigram_test_news_vectorizer.fit_transform(test_news['title'])\n",
    "bigram_countWordInTestNews = bigram_test_newscv.toarray()\n",
    "\n",
    "bigram_fake_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "bigram_fakeNewsCv = bigram_fake_vectorizer.fit_transform(fake_news['title'])\n",
    "bigram_countWordInFake = bigram_fakeNewsCv.toarray()\n",
    "\n",
    "bigram_real_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "bigram_realNewscv = bigram_real_vectorizer.fit_transform(real_news['title'])\n",
    "bigram_countWordInReal = bigram_realNewscv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables holds the word data in the order of counts of names in bigram form\n",
    "bigram_train_newsWords = bigram_train_news_vectorizer.get_feature_names()\n",
    "bigram_test_newsWords = bigram_test_news_vectorizer.get_feature_names()\n",
    "bigram_fakeWords = bigram_fake_vectorizer.get_feature_names()\n",
    "bigram_realWords = bigram_real_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part creates a structure that holds the number of occurences each word in entire data in bigram form\n",
    "bigram_totalCountsOfWordsInTrainNews = np.sum(bigram_countWordInTrainNews, axis=0)\n",
    "bigram_totalCountsOfWordsInTestNews = np.sum(bigram_countWordInTestNews, axis=0)\n",
    "bigram_totalCountsOfWordsInFake = np.sum(bigram_countWordInFake, axis=0)\n",
    "bigram_totalCountsOfWordsInReal = np.sum(bigram_countWordInReal, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in a label in bigram form\n",
    "bigram_length_fake, bigram_length_real = 0, 0\n",
    "for i in range(len(bigram_fakeWords)):\n",
    "    bigram_length_fake += bigram_totalCountsOfWordsInFake[i]\n",
    "\n",
    "for i in range(len(bigram_realWords)):\n",
    "    bigram_length_real += bigram_totalCountsOfWordsInReal[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_cond_probs_fake = cond_prob(bigram_fakeWords, bigram_totalCountsOfWordsInFake, bigram_length_fake, len(bigram_train_newsWords))\n",
    "bigram_cond_probs_real = cond_prob(bigram_realWords, bigram_totalCountsOfWordsInReal, bigram_length_real, len(bigram_train_newsWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_of_test = naive_bayes(bigram_test_news_vectorizer, bigram_countWordInTestNews, bigram_cond_probs_fake, bigram_cond_probs_real, bigram_fakeWords, bigram_realWords,\n",
    "            bigram_length_fake, bigram_length_real, len(bigram_train_newsWords), totalFakeProb, totalRealProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.26682692307693\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Accuracy of bigram part\n",
    "numOfTruePrediction = 0\n",
    "for i in range(len(labels_of_test)):\n",
    "    if test_news.iloc[i, 4] == labels_of_test[i]:\n",
    "        numOfTruePrediction += 1\n",
    "bigram_accuracy = 100*(numOfTruePrediction/float(len(labels_of_test)))\n",
    "print(bigram_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfIdf part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In  this part, I used TFIDF to take the the weights of the words. In other words, it returns the importance values of the words.\n",
    "tfidf_train_news_vectorizer = TfidfTransformer()\n",
    "tfidf_newscv = tfidf_train_news_vectorizer.fit_transform(newscv)\n",
    "tfidf_countWordInTrainNews = tfidf_newscv.toarray()\n",
    "\n",
    "tfidf_test_news_vectorizer = TfidfTransformer()\n",
    "tfidf_test_newscv = tfidf_test_news_vectorizer.fit_transform(test_newscv)\n",
    "tfidf_countWordInTestNews = tfidf_test_newscv.toarray()\n",
    "\n",
    "tfidf_fake_vectorizer = TfidfTransformer()\n",
    "tfidf_fakeNewsCv = tfidf_fake_vectorizer.fit_transform(fakeNewsCv)\n",
    "tfidf_countWordInFake = tfidf_fakeNewsCv.toarray()\n",
    "\n",
    "tfidf_real_vectorizer = TfidfTransformer()\n",
    "tfidf_realNewscv = tfidf_real_vectorizer.fit_transform(realNewscv)\n",
    "tfidf_countWordInReal = tfidf_realNewscv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_totalCountsOfWordsInTrainNews = np.sum(tfidf_countWordInTrainNews, axis=0)\n",
    "tfidf_totalCountsOfWordsInTestNews = np.sum(tfidf_countWordInTestNews, axis=0)\n",
    "tfidf_totalCountsOfWordsInFake = np.sum(tfidf_countWordInFake, axis=0)\n",
    "tfidf_totalCountsOfWordsInReal = np.sum(tfidf_countWordInReal, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 10 words whose presence most strongly predicts that the news is real.\n",
      "for\n",
      "of\n",
      "in\n",
      "breitbart\n",
      "trump\n",
      "to\n",
      "times\n",
      "york\n",
      "new\n",
      "the\n",
      "List of 10 words whose absence most strongly predicts that the news is real.\n",
      "infidel\n",
      "misogynistic\n",
      "immorality\n",
      "excerpt\n",
      "adrienne\n",
      "opinions\n",
      "whitlock\n",
      "nats\n",
      "strickland\n",
      "harper\n",
      "List of 10 words whose presence most strongly predicts that the news is fake.\n",
      "on\n",
      "for\n",
      "hillary\n",
      "and\n",
      "trump\n",
      "in\n",
      "of\n",
      "to\n",
      "the\n",
      "is\n",
      "no_title\n",
      "List of 10 words whose absence most strongly predicts that the news is fake.\n",
      "rothschild\n",
      "deviants\n",
      "nationalistic\n",
      "ordinanc\n",
      "ordinance\n",
      "waking\n",
      "insatiable\n",
      "algae\n",
      "tropical\n",
      "blooms\n"
     ]
    }
   ],
   "source": [
    "# The words whose presence has more weight to predict correctly in real news help us classifying.\n",
    "# Likewise, the words whose presence has less weight to predict correctly in fake news help us avoiding misclassification.\n",
    "\n",
    "indices_sorted_real_news = np.argsort(tfidf_totalCountsOfWordsInReal)\n",
    "indices_sorted_fake_news = np.argsort(tfidf_totalCountsOfWordsInFake)\n",
    "\n",
    "print(\"List of 10 words whose presence most strongly predicts that the news is real.\")\n",
    "for i in indices_sorted_real_news[-10:]:\n",
    "    if realWords[i] == 'no_title':\n",
    "        print(realWords[indices_sorted_real_news[-11]])\n",
    "    print(realWords[i])\n",
    "    \n",
    "print(\"List of 10 words whose absence most strongly predicts that the news is real.\")\n",
    "for i in indices_sorted_real_news[:10]:\n",
    "    if realWords[i] == 'no_title':\n",
    "        print(realWords[indices_sorted_real_news[11]])\n",
    "    print(realWords[i])\n",
    "    \n",
    "print(\"List of 10 words whose presence most strongly predicts that the news is fake.\")\n",
    "for i in indices_sorted_fake_news[-10:]:\n",
    "    if fakeWords[i] == 'no_title':\n",
    "        print(fakeWords[indices_sorted_fake_news[-11]])\n",
    "    print(fakeWords[i])\n",
    "    \n",
    "print(\"List of 10 words whose absence most strongly predicts that the news is fake.\")\n",
    "for i in indices_sorted_fake_news[:10]:\n",
    "    if fakeWords[i] == 'no_title':\n",
    "        print(fakeWords[indices_sorted_fake_news[11]])\n",
    "    print(fakeWords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_length_fake, tfidf_length_real = 0, 0\n",
    "for i in range(tfidf_countWordInFake.shape[1]):\n",
    "    tfidf_length_fake += tfidf_totalCountsOfWordsInFake[i]\n",
    "\n",
    "for i in range(tfidf_countWordInReal.shape[1]):\n",
    "    tfidf_length_real += tfidf_totalCountsOfWordsInReal[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_cond_probs_fake = cond_prob(fakeWords, tfidf_totalCountsOfWordsInFake, tfidf_length_fake, tfidf_countWordInTrainNews.shape[1])\n",
    "tfidf_cond_probs_real = cond_prob(realWords, tfidf_totalCountsOfWordsInReal, tfidf_length_real, tfidf_countWordInTrainNews.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_of_test = naive_bayes(test_news_vectorizer, tfidf_countWordInTestNews, tfidf_cond_probs_fake, tfidf_cond_probs_real, fakeWords, realWords,\n",
    "            tfidf_length_fake, tfidf_length_real, tfidf_countWordInTrainNews.shape[1], totalFakeProb, totalRealProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.21955128205127\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Accuracy of tfidf part\n",
    "numOfTruePrediction = 0\n",
    "for i in range(len(labels_of_test)):\n",
    "    if test_news.iloc[i, 4] == labels_of_test[i]:\n",
    "        numOfTruePrediction += 1\n",
    "tfidf_accuracy = 100*(numOfTruePrediction/float(len(labels_of_test)))\n",
    "print(tfidf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into consideration of stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using stop words speeds up the processing part. Because there will be less feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news_vectorizer = CountVectorizer(stop_words='english')\n",
    "newscv = train_news_vectorizer.fit_transform(train_news['title'])\n",
    "\n",
    "test_news_vectorizer = CountVectorizer(stop_words='english')\n",
    "test_newscv = test_news_vectorizer.fit_transform(test_news['title'])\n",
    "\n",
    "fake_vectorizer = CountVectorizer(stop_words='english')\n",
    "fakeNewsCv = fake_vectorizer.fit_transform(fake_news['title'])\n",
    "\n",
    "real_vectorizer = CountVectorizer(stop_words='english')\n",
    "realNewscv = real_vectorizer.fit_transform(real_news['title'])\n",
    "\n",
    "tfidf_train_news_vectorizer = TfidfTransformer()\n",
    "tfidf_newscv = tfidf_train_news_vectorizer.fit_transform(newscv)\n",
    "tfidf_countWordInTrainNews = tfidf_newscv.toarray()\n",
    "\n",
    "tfidf_test_news_vectorizer = TfidfTransformer()\n",
    "tfidf_test_newscv = tfidf_test_news_vectorizer.fit_transform(test_newscv)\n",
    "tfidf_countWordInTestNews = tfidf_test_newscv.toarray()\n",
    "\n",
    "tfidf_fake_vectorizer = TfidfTransformer()\n",
    "tfidf_fakeNewsCv = tfidf_fake_vectorizer.fit_transform(fakeNewsCv)\n",
    "tfidf_countWordInFake = tfidf_fakeNewsCv.toarray()\n",
    "\n",
    "tfidf_real_vectorizer = TfidfTransformer()\n",
    "tfidf_realNewscv = tfidf_real_vectorizer.fit_transform(realNewscv)\n",
    "tfidf_countWordInReal = tfidf_realNewscv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_totalCountsOfWordsInTrainNews = np.sum(tfidf_countWordInTrainNews, axis=0)\n",
    "tfidf_totalCountsOfWordsInTestNews = np.sum(tfidf_countWordInTestNews, axis=0)\n",
    "tfidf_totalCountsOfWordsInFake = np.sum(tfidf_countWordInFake, axis=0)\n",
    "tfidf_totalCountsOfWordsInReal = np.sum(tfidf_countWordInReal, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 10 words whose presence most strongly predicts that the news is real.\n",
      "rouhani\n",
      "nests\n",
      "brandenburg\n",
      "cked\n",
      "diversifies\n",
      "boys\n",
      "tina\n",
      "tapper\n",
      "wellesley\n",
      "murray\n",
      "List of 10 words whose absence most strongly predicts that the news is real.\n",
      "mengele\n",
      "imdb\n",
      "every\n",
      "industry\n",
      "birther\n",
      "craftsman\n",
      "criticism\n",
      "bratton\n",
      "satire\n",
      "editor\n",
      "List of 10 words whose presence most strongly predicts that the news is fake.\n",
      "altruistic\n",
      "fade\n",
      "richest\n",
      "ust\n",
      "mysterious\n",
      "eaters\n",
      "civilian\n",
      "headline\n",
      "tirol\n",
      "nathan\n",
      "List of 10 words whose absence most strongly predicts that the news is fake.\n",
      "urges\n",
      "obesity\n",
      "obese\n",
      "description\n",
      "retreat\n",
      "mother\n",
      "loch\n",
      "klux\n",
      "barbed\n",
      "canada\n"
     ]
    }
   ],
   "source": [
    "indices_sorted_real_news = np.argsort(tfidf_totalCountsOfWordsInReal)\n",
    "indices_sorted_fake_news = np.argsort(tfidf_totalCountsOfWordsInFake)\n",
    "\n",
    "print(\"List of 10 words whose presence most strongly predicts that the news is real.\")\n",
    "for i in indices_sorted_real_news[-10:]:\n",
    "    if realWords[i] == 'no_title':\n",
    "        print(realWords[indices_sorted_real_news[-11]])\n",
    "    print(realWords[i])\n",
    "    \n",
    "print(\"List of 10 words whose absence most strongly predicts that the news is real.\")\n",
    "for i in indices_sorted_real_news[:10]:\n",
    "    if realWords[i] == 'no_title':\n",
    "        print(realWords[indices_sorted_real_news[11]])\n",
    "    print(realWords[i])\n",
    "    \n",
    "print(\"List of 10 words whose presence most strongly predicts that the news is fake.\")\n",
    "for i in indices_sorted_fake_news[-10:]:\n",
    "    if fakeWords[i] == 'no_title':\n",
    "        print(fakeWords[indices_sorted_fake_news[-11]])\n",
    "    print(fakeWords[i])\n",
    "    \n",
    "print(\"List of 10 words whose absence most strongly predicts that the news is fake.\")\n",
    "for i in indices_sorted_fake_news[:10]:\n",
    "    if fakeWords[i] == 'no_title':\n",
    "        print(fakeWords[indices_sorted_fake_news[11]])\n",
    "    print(fakeWords[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4\n",
    "I calculated the accuracy in the ends of all parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, I developed a detection program of fake or real news by using probability with a bayesian approach. CountVectorizer and TfidfVectorizer from sklearn library helped so much. This functions saved me from sparse matrixes, and also it counted the words' occurences and frequency of occurences. After obtaining these important data, program calculates probability with naive bayes formula. So it gives the probability of the news is fake/real. Program decides the news label by the most probable label. I also calculated the log probability to prevent numerical underflow, and used Laplace smoothing in case of encountering never occurred words.\n",
    "\n",
    "$ P(w|c) = \\frac{count(w,c) + 1}{count(c) + |V|} $<br />\n",
    "Plus 1 and |V| came from Laplace smoothing. |V| denotes the unique words in data.\n",
    "\n",
    "Firstly, when I am trying to understand dataset, I used only title column. I extracted some special words. These words are pure to a label. I printed 3 examples of them.\n",
    "\n",
    "After understanding dataset, I tried to use text data to predict labels with implementing Naive Bayes. But my program needed so much time to give results and also gave MemoryError to hold that much data. After I consulted my TA, he advised me to give parameters for CountVectorizer. I gave different parameters to CountVectorizer, min_df and max_df. But this parameters caused to lose important part of data to predict correctly. So my predictions' accuracy is pretty low. In unigram part (which evaluate the words apartly from each others) I gave to CountVectorizermin_df=0.025, max_df=0.975 and it gave 62.379807692307686 accuracy. In bigram part (which evaluate the words in group of two words) I gave to CountVectorizer min_df=0.025, max_df=0.975 and it gave 66.08573717948718 accuracy. In tfidf part ( which gives a weighting scheme, that is the importance of words ), it gave 59.71554487179487 accuracy, also when I am finding 10 words whose presence/absence most strongly predicts that the news is fake/real news, I did not encounter with stop words because of min_df and max_df parameters. These accuracy values are so low. Also when I am using text data and calculating naive bayes, interpreter gave ZeroDivision Warning in log operation. I debugged my program and I observed that the warning came from the lines having so many different words. It multiplied the conditional probabilities and the value underflowed.\n",
    "\n",
    "After these low accuracies, I wanted to try to evaluate with only title data. With title data, I didn't need to ignore the words most and least frequent by using min_df and max_df parameters. In this part, my program gave pretty good accuracy. In unigram part, the accuracy is 89.90384615384616. In bigram part, the accuracy 79.26682692307693. In Tfidf part, the accuracy 87.21955128205127.\n",
    "\n",
    "When using text data in CountVectorizer, I did not encounter with stop words, because I ignored these terms in min_df and max_df parameters. But to be able to see the effect of stop words I again calculated the \"10 words whose presence/absence most strongly predicts that the news is real/fake\" in the part where I used only title data. In this part I used the stop_words='english' parameter in CountVectorizer. Using this parameter gave me more exact results for these 10 words.\n",
    "\n",
    "In last part, I calculated accuracy of all the methods(Unigram, Bigram, TfIdf). These results are right after each part to give more compact representation.\n",
    "\n",
    "To point a moral, in the part where I take more accurate results, the unigram model gave me more accurate models. I believed that bigram models would give more accuracy in any way, but my program gave more accuracy in unigram. Also TfIdf vectorizer so much helped to extract meaningful information from data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
